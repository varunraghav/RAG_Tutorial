{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44dbdc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your Open API key: ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = getpass.getpass(\"Enter your Open API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c35eb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your Llama Cloud API key: ········\n"
     ]
    }
   ],
   "source": [
    "# Use getpass.getpass() to prompt for the API key securely\n",
    "os.environ[\"LLAMA_CLOUD_API_KEY\"] = getpass.getpass(\"Enter your Llama Cloud API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b850768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your Spider API key: ········\n"
     ]
    }
   ],
   "source": [
    "spider_api_key = getpass.getpass(\"Enter your Spider API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9198e7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "from llama_index.core import SummaryIndex\n",
    "from llama_index.core import Document\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.core.postprocessor import KeywordNodePostprocessor\n",
    "\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "from llama_index.core.agent import ReActAgent\n",
    "\n",
    "\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "from llama_index.readers.web import SpiderWebReader\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from llama_parse import LlamaParse\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "import nest_asyncio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "208dde9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSIST_DIR = './new_storage'\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    documents_United_Full = SimpleDirectoryReader('United').load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents_United_Full)\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac108197",
   "metadata": {},
   "outputs": [],
   "source": [
    "spider_reader = SpiderWebReader(\n",
    "    api_key=spider_api_key,  # Get one at https://spider.cloud\n",
    "    mode=\"scrape\",\n",
    "    # params={} # Optional parameters see more on https://spider.cloud/docs/api\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91b824d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_united = spider_reader.load_data(url=\"https://r.jina.ai/https://www.united.com/en/us/fly/baggage/carry-on-bags.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6af77ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_delta = spider_reader.load_data(url=\"https://r.jina.ai/https://www.delta.com/us/en/baggage/carry-on-baggage?srsltid=AfmBOopJ1ha7OEiwm46qaLxEQ_tvi6lrtK7NOxt0dttocsarTY1-pm1V\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c87701ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_american = spider_reader.load_data(url=\"https://r.jina.ai/https://www.aa.com/i18n/travel-info/baggage/carry-on-baggage.jsp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ff2307d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the persistence directory and PDF file name\n",
    "PERSIST_DIR = './new_storage_polity'\n",
    "PDF_FILE = 'Indian_Polity_Laxmi-Kant-6th-Edition-.pdf'\n",
    "\n",
    "    \n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    reader = PdfReader(pdf_file)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    # Extract text from the PDF\n",
    "    pdf_text = extract_text_from_pdf(PDF_FILE)\n",
    "    \n",
    "    # Create a Document object\n",
    "    document_polity = Document(pdf_text)\n",
    "    \n",
    "    # Build the index\n",
    "    index_polity = VectorStoreIndex.from_documents([document_polity])\n",
    "    index_polity.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    # Load the index from the existing persistence directory\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index_polity = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b57d12",
   "metadata": {},
   "source": [
    "# HyDE\n",
    "\n",
    "HyDE stands for Hypothetical Document Embeddings. It's an advanced technique used in Retrieval-Augmented Generation (RAG) or other information retrieval systems to enhance the relevance of retrieved documents.\n",
    "\n",
    "Instead of searching the database directly with the original query, the system uses a large language model (LLM) to generate a hypothetical answer or response to the query. This answer isn't necessarily accurate or grounded but serves as a guide for retrieval.\n",
    "\n",
    "The generated hypothetical answer is embedded into a vector space using the same embedding model applied to the database of documents.\n",
    "\n",
    "Similarity Matching: The system retrieves documents from the database that are semantically similar to the hypothetical answer. This step improves the chance of finding documents relevant to the user's intent, even if the original query wasn't perfect.\n",
    "\n",
    "##### Helpful to answer vague sounding questions where llm could provide a better contest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5849ac17",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"criticism of supreme court?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4bac550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>The criticism of the Supreme Court includes concerns about the delays in delivering judgments, the need for a more efficient clearance of pending cases, the suggestion to introduce plea-bargaining for decriminalization, and the proposal to bring down the hierarchy of subordinate courts to a two-tier system under the High Court.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_engine = index_polity.as_query_engine()\n",
    "response = query_engine.query(query_str)\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b62292e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>Criticism of the Supreme Court includes concerns about delays in decision-making, the potential for passing on the responsibility of strong decision-making to the courts, failure by the legislature and executive to protect basic rights of citizens, misuse of the court by authoritarian parliamentary party governments for ulterior motives, and the court potentially succumbing to human weaknesses such as a desire for populism, publicity, and media attention. Additionally, criticism has been directed at trends like expansion of judicial control over discretionary powers, excessive delegation without limitation, indiscriminate exercise of contempt power, passing orders that are unworkable, and overextending standard rules of interpretation in pursuit of economic, social, and educational objectives.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
    "from llama_index.core.query_engine import TransformQueryEngine\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "hyde = HyDEQueryTransform(include_original=True)\n",
    "hyde_query_engine = TransformQueryEngine(query_engine, hyde)\n",
    "response = hyde_query_engine.query(query_str)\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0778165b",
   "metadata": {},
   "source": [
    "# 2. MultiQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8be0139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set Logging to DEBUG for more detailed outputs\n",
    "from llama_index.core.query_engine import MultiStepQueryEngine\n",
    "\n",
    "from llama_index.core.indices.query.query_transform.base import (\n",
    "    StepDecomposeQueryTransform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf7ea67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM (gpt-4)\n",
    "gpt4 = OpenAI(temperature=0, model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9d98f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt-4\n",
    "step_decompose_transform = StepDecomposeQueryTransform(llm=gpt4, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3777bfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_summary = \"Used to answer questions about the author\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef654907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Turn the index into a query engine\n",
    "query_engine = index_polity.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3e4b79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: The process of writing the Constitution involved several key events, starting with the historical background that led to its creation. The making of the Constitution included drafting sessions from May 1946 to November 1949, with notable sessions in May-June 1949, July-September 1949, October 1949, and November 1949. The Assembly reconvened on January 24, 1950, for the members to sign the Constitution of India. Comparatively, other countries took varying lengths of time to draft their constitutions, such as the USA in less than 4 months, Canada in about 2 years and 6 months, Australia in about 9 years, and South Africa in 1 year.\n",
      "Follow-up: The process of writing the Constitution involved drafting sessions from May 1946 to November 1949, with key sessions in May-June 1949, July-September 1949, October 1949, and November 1949. The Assembly reconvened on January 24, 1950, for the members to sign the Constitution of India. Comparatively, other countries took varying lengths of time to draft their constitutions, such as the USA in less than 4 months, Canada in about 2 years and 6 months, Australia in about 9 years, and South Africa in 1 year.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Ask a broad question\n",
    "response1 = query_engine.query(\"Give me a short summary of the key events behind Consitution writing.\")\n",
    "summary = response1.response.strip()\n",
    "print(\"Summary:\", summary)\n",
    "\n",
    "# Step 2: Use the summary to refine your next query\n",
    "followup_question = f\"Based on this summary: {summary}\\nCould you clarify the timeline of events?\"\n",
    "response2 = query_engine.query(followup_question)\n",
    "print(\"Follow-up:\", response2.response.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c66a2f",
   "metadata": {},
   "source": [
    "## MultiQuery for Polity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dfb7074a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import ServiceContext, PromptTemplate, Document, VectorStoreIndex\n",
    "\n",
    "# Correct instantiation of OpenAI LLM for LlamaIndex\n",
    "Settings.llm = OpenAI(model_name=\"gpt-4\", temperature=0.5)\n",
    "\n",
    "# Define the multi-query generation prompt\n",
    "multi_query_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "You are an AI assistant. Generate FIVE different perspectives or versions of the user's query \n",
    "to enhance document retrieval. These variations should provide diverse ways to frame the question.\n",
    "\n",
    "Original question: {question}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "def generate_query_variations(user_query):\n",
    "    \"\"\"Generate multiple variations of the input query.\"\"\"\n",
    "    # Get the response object\n",
    "    response = Settings.llm.complete(multi_query_prompt.format(question=user_query))\n",
    "    \n",
    "    # Extract text from the response and split into variations\n",
    "    variations = [q.strip() for q in response.text.split(\"\\n\") if q.strip()]\n",
    "    \n",
    "    # Ensure we have exactly 5 variations\n",
    "    return variations[:5]\n",
    "\n",
    "def multi_query_pipeline(user_query, base_index):\n",
    "    \"\"\"\n",
    "    Execute multi-query retrieval and synthesis pipeline.\n",
    "    \n",
    "    Args:\n",
    "        user_query (str): Original user query\n",
    "        base_index (VectorStoreIndex): Base document index\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (query_variations, final_response)\n",
    "    \"\"\"\n",
    "    # Step 1: Generate multiple query variations\n",
    "    query_variations = generate_query_variations(user_query)\n",
    "    \n",
    "    # Step 2: Retrieve documents for each query variation\n",
    "    query_engine = base_index.as_query_engine()\n",
    "    combined_docs = set()  # Use a set to deduplicate documents\n",
    "    \n",
    "    for query in query_variations:\n",
    "        response = query_engine.query(query)\n",
    "        for node in response.source_nodes:\n",
    "            combined_docs.add(node.node.get_content())\n",
    "    \n",
    "    # Step 3: Build a temporary index with deduplicated documents\n",
    "    # Fix: Create Document objects correctly with text parameter\n",
    "    documents = [Document(text=doc) for doc in combined_docs]\n",
    "    \n",
    "    ephemeral_index = VectorStoreIndex.from_documents(\n",
    "        documents,\n",
    "        setting = Settings\n",
    "    )\n",
    "    \n",
    "    # Step 4: Query the ephemeral index for the final response\n",
    "    ephemeral_engine = ephemeral_index.as_query_engine()\n",
    "    final_response = ephemeral_engine.query(user_query)\n",
    "    \n",
    "    return query_variations, final_response.response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8b37b988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query: criticism of indian supreme court\n",
      "\n",
      "=== Query Variations ===\n",
      "1. 1. Analysis of the Indian Supreme Court's shortcomings\n",
      "2. 2. Evaluating the drawbacks of the Indian Supreme Court\n",
      "3. 3. Unfavorable opinions on the Indian Supreme Court\n",
      "4. 4. Dissecting the faults of the Indian Supreme Court\n",
      "5. 5. Examining the criticisms directed towards the Indian Supreme Court\n",
      "\n",
      "=== Final Synthesized Answer ===\n",
      "The criticism of the Indian Supreme Court has been centered around the appointment process of judges. There have been concerns raised regarding the lack of transparency and accountability in the appointment of judges, especially with the collegium system. Additionally, the issue of judicial overreach and encroachment on the powers of the executive and legislature has also been a point of criticism. Some critics argue that the Supreme Court's decisions have sometimes been influenced by personal biases or external pressures, rather than purely based on legal principles.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "user_query = input(\"Enter your query: \")\n",
    "\n",
    "# Run the pipeline\n",
    "query_variations, final_answer = multi_query_pipeline(user_query, index_polity)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n=== Query Variations ===\")\n",
    "for i, query in enumerate(query_variations, start=1):\n",
    "    print(f\"{i}. {query}\")\n",
    "\n",
    "print(\"\\n=== Final Synthesized Answer ===\")\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d46062",
   "metadata": {},
   "source": [
    "#### Using Airlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d218e5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query: Pet policy of united airlines?\n",
      "\n",
      "=== Query Variations ===\n",
      "1. 1. What are the regulations regarding pets on United Airlines flights?\n",
      "2. 2. Can you provide information on United Airlines' pet policy?\n",
      "3. 3. How does United Airlines handle pets on their flights?\n",
      "4. 4. What are the rules and restrictions for flying with pets on United Airlines?\n",
      "5. 5. Could you explain United Airlines' guidelines for traveling with pets?\n",
      "\n",
      "=== Final Synthesized Answer ===\n",
      "United Airlines allows pets to fly in the cabin with their owners, with certain restrictions based on the type of plane. Passengers can bring up to 2 pets per person on most flights, with specific limitations on certain aircraft. Pets must be either a cat or a dog and must travel in a carrier that fits under the seat in front of the passenger. United Airlines does not allow pets to fly to certain states and countries, and there are specific rules regarding pet travel paperwork and vaccinations for international flights. Additionally, there are fees associated with traveling with pets on United Airlines, with different rates depending on the date of ticket purchase.\n"
     ]
    }
   ],
   "source": [
    "user_query = input(\"Enter your query: \")\n",
    "\n",
    "# Run the pipeline\n",
    "query_variations, final_answer = multi_query_pipeline(user_query, index)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n=== Query Variations ===\")\n",
    "for i, query in enumerate(query_variations, start=1):\n",
    "    print(f\"{i}. {query}\")\n",
    "\n",
    "print(\"\\n=== Final Synthesized Answer ===\")\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b30a51a",
   "metadata": {},
   "source": [
    "# 3. Stepback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6ac2a3",
   "metadata": {},
   "source": [
    "The approach focuses on producing \"step-back\" questions. These questions are designed to abstract from the original question and encourage the model to consider broader or related contexts to improve answer quality.\n",
    "\n",
    "\"At year saw the creation of the region where the county of Hertfordshire is located?\"\n",
    "Step-Back Question: A more abstracted version that explores the broader context or reframes the question. For example:\n",
    "\n",
    "\"Which region is the county of Hertfordshire located?\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bcabe557",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import ServiceContext, PromptTemplate, Document, VectorStoreIndex\n",
    "from typing import List, Dict\n",
    "\n",
    "# Step-back prompting examples\n",
    "STEPBACK_EXAMPLES = [\n",
    "    {\n",
    "        \"input\": \"Why is SoftPro not saving my project files?\",\n",
    "        \"output\": \"What are common reasons for SoftPro failing to save files?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Why can't I install SoftPro on my device?\",\n",
    "        \"output\": \"What are common issues when installing SoftPro?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Why is my SoftPro license key not working?\",\n",
    "        \"output\": \"What are common issues with SoftPro license keys?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "        \"output\": \"what can the members of The Police do?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindel's was born in what country?\",\n",
    "        \"output\": \"what is Jan Sindel's personal history?\",\n",
    "    }\n",
    "]\n",
    "\n",
    "def format_examples(examples: List[Dict[str, str]]) -> str:\n",
    "    \"\"\"Format the examples for the prompt.\"\"\"\n",
    "    formatted = \"\"\n",
    "    for ex in examples:\n",
    "        formatted += f\"Specific Question: {ex['input']}\\n\"\n",
    "        formatted += f\"General Question: {ex['output']}\\n\\n\"\n",
    "    return formatted.strip()\n",
    "\n",
    "# Step-back prompting template\n",
    "stepback_prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an AI assistant that helps users step back from specific questions to more general questions that capture the broader context.\n",
    "Given a specific question, generate a more general question that will help provide better context for answering the original question.\n",
    "\n",
    "Here are some examples:\n",
    "\n",
    "{examples}\n",
    "\n",
    "Now, please generate a more general question for:\n",
    "Specific Question: {question}\n",
    "General Question:\"\"\"\n",
    ")\n",
    "\n",
    "# Multi-query generation prompt (from previous implementation)\n",
    "multi_query_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "You are an AI assistant. Generate FIVE different perspectives or versions of the user's query \n",
    "to enhance document retrieval. These variations should provide diverse ways to frame the question.\n",
    "Original question: {question}\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "938d218c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_stepback_question(question: str, examples: List[Dict[str, str]] = STEPBACK_EXAMPLES) -> str:\n",
    "    \"\"\"Generate a more general 'step-back' question from the original question.\"\"\"\n",
    "    formatted_examples = format_examples(examples)\n",
    "    prompt = stepback_prompt.format(examples=formatted_examples, question=question)\n",
    "    \n",
    "    # Get the response from LLM\n",
    "    response = Settings.llm.complete(prompt)\n",
    "    return response.text.strip()\n",
    "\n",
    "def generate_query_variations(user_query: str) -> List[str]:\n",
    "    \"\"\"Generate multiple variations of the input query.\"\"\"\n",
    "    response = Settings.llm.complete(multi_query_prompt.format(question=user_query))\n",
    "    variations = [q.strip() for q in response.text.split(\"\\n\") if q.strip()]\n",
    "    return variations[:5]\n",
    "\n",
    "def stepback_query_pipeline(user_query: str, base_index: VectorStoreIndex) -> tuple:\n",
    "    \"\"\"\n",
    "    Execute the step-back query pipeline with multi-query retrieval and synthesis.\n",
    "    \n",
    "    Args:\n",
    "        user_query (str): Original user query\n",
    "        base_index (VectorStoreIndex): Base document index\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (stepback_question, query_variations, final_response)\n",
    "    \"\"\"\n",
    "    # Step 1: Generate step-back question\n",
    "    stepback_question = generate_stepback_question(user_query)\n",
    "    \n",
    "    # Step 2: Generate variations of the step-back question\n",
    "    query_variations = generate_query_variations(stepback_question)\n",
    "    \n",
    "    # Step 3: Retrieve documents for each query variation\n",
    "    query_engine = base_index.as_query_engine()\n",
    "    combined_docs = set()\n",
    "    \n",
    "    # Include both original and step-back questions in retrieval\n",
    "    all_queries = [user_query, stepback_question] + query_variations\n",
    "    \n",
    "    for query in all_queries:\n",
    "        response = query_engine.query(query)\n",
    "        for node in response.source_nodes:\n",
    "            combined_docs.add(node.node.get_content())\n",
    "    \n",
    "    # Step 4: Build temporary index with deduplicated documents\n",
    "    documents = [Document(text=doc) for doc in combined_docs]\n",
    "    ephemeral_index = VectorStoreIndex.from_documents(\n",
    "        documents,\n",
    "        setting=Settings\n",
    "    )\n",
    "    \n",
    "    # Step 5: Query the ephemeral index with the original question\n",
    "    ephemeral_engine = ephemeral_index.as_query_engine()\n",
    "    final_response = ephemeral_engine.query(user_query)\n",
    "    \n",
    "    return stepback_question, query_variations, final_response.response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a9987db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query: I flew last week from Delhi to San Francisco on United first class. I had 2 check-in bags and 2 carryon bags that haven’t been delivered yet. What should I do?\n",
      "\n",
      "=== Step-Back Question ===\n",
      "What are common steps to take when luggage is lost or delayed after a flight?\n",
      "\n",
      "=== Query Variations ===\n",
      "1. 1. How can I address the situation when my luggage goes missing or is delayed following a flight?\n",
      "2. 2. What should I do if my luggage is lost or delayed upon arrival from a flight?\n",
      "3. 3. What are the typical procedures to follow if my luggage is misplaced or delayed after a flight?\n",
      "4. 4. In the event that my luggage is lost or delayed post-flight, what steps should I take?\n",
      "5. 5. How can I handle the situation of lost or delayed luggage after flying?\n",
      "\n",
      "=== Final Synthesized Answer ===\n",
      "You should contact the airline you flew with, United Airlines, to report the missing bags. They will be able to assist you in locating and delivering your missing luggage.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "user_query = input(\"Enter your query: \")\n",
    "\n",
    "# Run the pipeline\n",
    "stepback_q, variations, answer = stepback_query_pipeline(user_query, index_polity)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n=== Step-Back Question ===\")\n",
    "print(stepback_q)\n",
    "\n",
    "print(\"\\n=== Query Variations ===\")\n",
    "for i, query in enumerate(variations, start=1):\n",
    "    print(f\"{i}. {query}\")\n",
    "\n",
    "print(\"\\n=== Final Synthesized Answer ===\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd126951",
   "metadata": {},
   "source": [
    "# 4. Query Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4cdd4ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import ServiceContext, PromptTemplate, Document, VectorStoreIndex\n",
    "from typing import List, Dict, Tuple\n",
    "import re\n",
    "\n",
    "# Query decomposition prompt template\n",
    "decomposition_prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an AI assistant that helps break down complex questions into simpler sub-questions.\n",
    "Each sub-question should:\n",
    "1. Focus on a specific aspect of the main question\n",
    "2. Be self-contained and answerable independently\n",
    "3. Together, cover all aspects of the original question\n",
    "\n",
    "For example:\n",
    "Complex Question: \"What were the major economic and social impacts of the 2008 financial crisis in the United States?\"\n",
    "Sub-questions:\n",
    "1. What were the immediate economic effects of the 2008 financial crisis on the US economy?\n",
    "2. How did the 2008 financial crisis affect unemployment rates in the United States?\n",
    "3. What impact did the 2008 financial crisis have on the US housing market?\n",
    "4. How did the 2008 financial crisis affect American households' wealth and savings?\n",
    "5. What social changes occurred in American society as a result of the 2008 financial crisis?\n",
    "\n",
    "Now, please break down the following question into 3-5 specific sub-questions:\n",
    "Complex Question: {question}\n",
    "Sub-questions:\"\"\"\n",
    ")\n",
    "\n",
    "# Synthesis prompt template\n",
    "synthesis_prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an AI assistant helping to synthesize multiple pieces of information into a coherent answer.\n",
    "\n",
    "Original Question: {original_question}\n",
    "\n",
    "Here are the answers to related sub-questions:\n",
    "\n",
    "{subquestion_answers}\n",
    "\n",
    "Please provide a comprehensive answer to the original question by synthesizing these pieces of information.\n",
    "Make sure to:\n",
    "1. Address all aspects of the original question\n",
    "2. Maintain logical flow between different pieces of information\n",
    "3. Avoid redundancy\n",
    "4. Provide a coherent narrative\n",
    "\n",
    "Synthesized Answer:\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "68de3e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_sub_questions(question: str) -> List[str]:\n",
    "    \"\"\"Generate sub-questions from a complex question.\"\"\"\n",
    "    response = Settings.llm.complete(decomposition_prompt.format(question=question))\n",
    "    \n",
    "    # Split the response into individual questions and clean them\n",
    "    sub_questions = []\n",
    "    for line in response.text.split('\\n'):\n",
    "        # Remove leading numbers and dots\n",
    "        line = re.sub(r'^\\d+\\.\\s*', '', line.strip())\n",
    "        if line and '?' in line:  # Ensure it's a question\n",
    "            sub_questions.append(line)\n",
    "    \n",
    "    return sub_questions\n",
    "\n",
    "def query_index_with_questions(questions: List[str], \n",
    "                             base_index: VectorStoreIndex) -> List[Tuple[str, str]]:\n",
    "    \"\"\"Query the index with multiple questions and return question-answer pairs.\"\"\"\n",
    "    query_engine = base_index.as_query_engine()\n",
    "    results = []\n",
    "    \n",
    "    for question in questions:\n",
    "        response = query_engine.query(question)\n",
    "        results.append((question, response.response))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def format_subquestion_answers(qa_pairs: List[Tuple[str, str]]) -> str:\n",
    "    \"\"\"Format question-answer pairs for the synthesis prompt.\"\"\"\n",
    "    formatted = \"\"\n",
    "    for i, (question, answer) in enumerate(qa_pairs, 1):\n",
    "        formatted += f\"Sub-question {i}: {question}\\n\"\n",
    "        formatted += f\"Answer {i}: {answer}\\n\\n\"\n",
    "    return formatted.strip()\n",
    "\n",
    "def synthesize_answers(original_question: str, \n",
    "                      qa_pairs: List[Tuple[str, str]]) -> str:\n",
    "    \"\"\"Synthesize multiple answers into a coherent response.\"\"\"\n",
    "    formatted_answers = format_subquestion_answers(qa_pairs)\n",
    "    prompt = synthesis_prompt.format(\n",
    "        original_question=original_question,\n",
    "        subquestion_answers=formatted_answers\n",
    "    )\n",
    "    \n",
    "    response = Settings.llm.complete(prompt)\n",
    "    return response.text.strip()\n",
    "\n",
    "def query_decomposition_pipeline(question: str, \n",
    "                               base_index: VectorStoreIndex) -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Execute the complete query decomposition pipeline.\n",
    "    \n",
    "    Args:\n",
    "        question (str): Original complex question\n",
    "        base_index (VectorStoreIndex): Base document index\n",
    "    \n",
    "    Returns:\n",
    "        Dict containing sub-questions, individual answers, and final synthesis\n",
    "    \"\"\"\n",
    "    # Step 1: Generate sub-questions\n",
    "    sub_questions = generate_sub_questions(question)\n",
    "    \n",
    "    # Step 2: Query index with sub-questions\n",
    "    qa_pairs = query_index_with_questions(sub_questions, base_index)\n",
    "    \n",
    "    # Step 3: Synthesize final answer\n",
    "    final_answer = synthesize_answers(question, qa_pairs)\n",
    "    \n",
    "    # Return all components for analysis\n",
    "    return {\n",
    "        'original_question': question,\n",
    "        'sub_questions': sub_questions,\n",
    "        'qa_pairs': qa_pairs,\n",
    "        'final_answer': final_answer\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "631e1536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your complex question: pet policy of various airlines?\n",
      "\n",
      "=== Sub-Questions ===\n",
      "1. What are the specific pet policies of major airlines in terms of size and weight restrictions for pets traveling in the cabin?\n",
      "2. How do different airlines handle pet fees and requirements for documentation, such as health certificates or vaccination records?\n",
      "3. What are the rules and regulations regarding pet travel in the cargo hold for each airline, including temperature restrictions and availability on different aircraft?\n",
      "4. How do airlines accommodate service animals or emotional support animals, and what are the specific guidelines and documentation required for these types of pets?\n",
      "5. Are there any specific restrictions or guidelines for international pet travel on different airlines, such as quarantine regulations or breed restrictions?\n",
      "\n",
      "=== Individual Answers ===\n",
      "\n",
      "Sub-question 1: What are the specific pet policies of major airlines in terms of size and weight restrictions for pets traveling in the cabin?\n",
      "Answer: There are no weight or breed limitations for pets traveling in the cabin on major airlines. However, pets must travel in either a hard-sided or soft-sided carrier that fits under the seat in front of you.\n",
      "\n",
      "Sub-question 2: How do different airlines handle pet fees and requirements for documentation, such as health certificates or vaccination records?\n",
      "Answer: Different airlines handle pet fees and requirements for documentation, such as health certificates or vaccination records, in various ways. Some airlines charge a fee for traveling with pets, which can range from $125 to $150 each way, depending on when the ticket was purchased. In terms of documentation, airlines typically require rabies and health certificates for dogs and cats traveling internationally. The rabies vaccinations must be completed at least 28 days before travel. Additionally, for domestic flights, puppies and kittens must meet certain age requirements to fly. It is important to check with each airline for their specific pet policies and requirements before traveling with your pet.\n",
      "\n",
      "Sub-question 3: What are the rules and regulations regarding pet travel in the cargo hold for each airline, including temperature restrictions and availability on different aircraft?\n",
      "Answer: Pets can no longer be checked through the PetSafe program and can only fly in cargo when traveling with certain active-duty military or State Department employees. There are restrictions on the number of pets allowed per person, with most flights allowing up to 2 pets per person. Some aircraft have limited space under the middle seat, allowing only one pet per person. Different aircraft have varying restrictions on pet travel, with specific rules for where pets can be placed in the cabin. Additionally, certain countries and states have restrictions on pet travel, with a list of places where United does not allow pets to fly to, from, or through.\n",
      "\n",
      "Sub-question 4: How do airlines accommodate service animals or emotional support animals, and what are the specific guidelines and documentation required for these types of pets?\n",
      "Answer: Airlines allow service animals to accompany passengers in the cabin free of charge. For emotional support animals, they must be transported as pets and follow the guidelines for pet travel, including being placed in a carrier that fits under the seat. Specific documentation required includes health certificates, proof of recent vaccinations, and a certificate of health from a veterinarian. Service animals are an exception and are not required to travel in a carrier.\n",
      "\n",
      "Sub-question 5: Are there any specific restrictions or guidelines for international pet travel on different airlines, such as quarantine regulations or breed restrictions?\n",
      "Answer: There are specific guidelines for international pet travel on different airlines, including requirements for rabies and health certificates for dogs and cats traveling internationally on United. Rabies vaccinations must be completed at least 28 days before travel for both dogs and cats. Additionally, there are rules for traveling to the U.S. from countries considered high risk for rabies, with close adherence to CDC guidelines on specific country requirements. It is recommended to check these guidelines at least two months before the trip. For United flights, there are restrictions on where you can sit with your pet on the plane based on the type of aircraft. Each plane has limits on the total number of pets allowed.\n",
      "\n",
      "=== Final Synthesized Answer ===\n",
      "The pet policies of major airlines vary in terms of size and weight restrictions for pets traveling in the cabin. There are no weight or breed limitations, but pets must travel in a carrier that fits under the seat. Different airlines have varying fees and requirements for documentation such as health certificates and vaccination records. Fees can range from $125 to $150 each way, and rabies and health certificates are required for international travel. Some airlines allow pets in the cargo hold under specific circumstances, with restrictions on the number of pets per person and limitations on certain aircraft. Service animals are accommodated in the cabin free of charge, while emotional support animals must travel as pets. International pet travel has specific guidelines, including rabies vaccinations and adherence to CDC guidelines for high-risk countries. It is important to check with each airline for their specific policies before traveling with your pet.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example complex question\n",
    "user_query = input(\"Enter your complex question: \")\n",
    "\n",
    "# Run the pipeline\n",
    "results = query_decomposition_pipeline(user_query, index)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n=== Sub-Questions ===\")\n",
    "for i, question in enumerate(results['sub_questions'], 1):\n",
    "    print(f\"{i}. {question}\")\n",
    "\n",
    "print(\"\\n=== Individual Answers ===\")\n",
    "for i, (question, answer) in enumerate(results['qa_pairs'], 1):\n",
    "    print(f\"\\nSub-question {i}: {question}\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "\n",
    "print(\"\\n=== Final Synthesized Answer ===\")\n",
    "print(results['final_answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7afce1",
   "metadata": {},
   "source": [
    "# 5. RagFusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4b50214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import ServiceContext, PromptTemplate, Document, VectorStoreIndex\n",
    "from typing import List, Dict, Tuple, Set\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "class RAGFusion:\n",
    "    def __init__(self, base_index: VectorStoreIndex, k: int = 10):\n",
    "        \"\"\"\n",
    "        Initialize RAGFusion.\n",
    "        \n",
    "        Args:\n",
    "            base_index: Base vector store index\n",
    "            k: Number of top documents to retrieve per query\n",
    "        \"\"\"\n",
    "        self.base_index = base_index\n",
    "        self.k = k\n",
    "        \n",
    "        # Hyperparameter for RRF scoring (typically 60)\n",
    "        self.rrf_k = 60\n",
    "        \n",
    "        # Initialize prompt templates\n",
    "        self.variation_prompt = PromptTemplate(\n",
    "            template=\"\"\"Generate THREE different versions of the given query that capture different aspects \n",
    "            or ways of expressing the same information need. Make the variations semantically diverse.\n",
    "\n",
    "Original query: {query}\n",
    "\n",
    "Generate variations, one per line:\"\"\"\n",
    "        )\n",
    "        \n",
    "        self.generation_prompt = PromptTemplate(\n",
    "            template=\"\"\"Based on the given context, provide a comprehensive answer to the question.\n",
    "If the context doesn't contain enough information, say so.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        )\n",
    "\n",
    "    def generate_query_variations(self, query: str) -> List[str]:\n",
    "        \"\"\"Generate semantically diverse variations of the input query.\"\"\"\n",
    "        response = Settings.llm.complete(self.variation_prompt.format(query=query))\n",
    "        variations = [q.strip() for q in response.text.split('\\n') if q.strip()]\n",
    "        # Include original query\n",
    "        return [query] + variations[:3]  # Limit to 3 variations + original\n",
    "\n",
    "    def retrieve_documents(self, query: str) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Retrieve documents for a single query with similarity scores.\n",
    "        Returns list of (doc_content, score) tuples.\n",
    "        \"\"\"\n",
    "        query_engine = self.base_index.as_query_engine(similarity_top_k=self.k)\n",
    "        response = query_engine.query(query)\n",
    "        \n",
    "        # Extract documents and scores\n",
    "        results = []\n",
    "        for node in response.source_nodes:\n",
    "            doc_content = node.node.get_content()\n",
    "            score = node.score if node.score is not None else 0.0\n",
    "            results.append((doc_content, score))\n",
    "            \n",
    "        return results\n",
    "\n",
    "    def reciprocal_rank_fusion(self, \n",
    "                             all_results: List[List[Tuple[str, float]]], \n",
    "                             k: int = 60) -> List[str]:\n",
    "        \"\"\"\n",
    "        Implement Reciprocal Rank Fusion to combine multiple ranked lists.\n",
    "        \n",
    "        Args:\n",
    "            all_results: List of ranked lists, each containing (doc_content, score) tuples\n",
    "            k: RRF constant (default 60)\n",
    "            \n",
    "        Returns:\n",
    "            Combined and reranked list of documents\n",
    "        \"\"\"\n",
    "        rrf_scores = defaultdict(float)\n",
    "        \n",
    "        # Calculate RRF scores\n",
    "        for ranked_list in all_results:\n",
    "            for rank, (doc_content, _) in enumerate(ranked_list, start=1):\n",
    "                rrf_scores[doc_content] += 1.0 / (k + rank)\n",
    "        \n",
    "        # Sort documents by RRF score\n",
    "        sorted_docs = sorted(rrf_scores.items(), \n",
    "                           key=lambda x: x[1], \n",
    "                           reverse=True)\n",
    "        \n",
    "        # Return just the documents (without scores)\n",
    "        return [doc for doc, _ in sorted_docs]\n",
    "\n",
    "    def execute_fusion(self, query: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Execute the complete RAGFusion pipeline.\n",
    "        \n",
    "        Args:\n",
    "            query: User's query\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing all intermediate results and final answer\n",
    "        \"\"\"\n",
    "        # Step 1: Generate query variations\n",
    "        query_variations = self.generate_query_variations(query)\n",
    "        \n",
    "        # Step 2: Retrieve documents for each query variation\n",
    "        all_results = []\n",
    "        for q in query_variations:\n",
    "            results = self.retrieve_documents(q)\n",
    "            all_results.append(results)\n",
    "        \n",
    "        # Step 3: Combine results using RRF\n",
    "        fused_docs = self.reciprocal_rank_fusion(all_results, k=self.rrf_k)\n",
    "        \n",
    "        # Step 4: Create context from top fused documents\n",
    "        context = \"\\n\\n\".join(fused_docs[:self.k])  # Use top k documents\n",
    "        \n",
    "        # Step 5: Generate final answer\n",
    "        final_response = Settings.llm.complete(\n",
    "            self.generation_prompt.format(\n",
    "                question=query,\n",
    "                context=context\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'query_variations': query_variations,\n",
    "            'fused_documents': fused_docs[:self.k],\n",
    "            'final_answer': final_response.text.strip()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4cd56b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query: criticism of Indian Supreme Court\n",
      "\n",
      "=== Query Variations ===\n",
      "1. criticism of Indian Supreme Court\n",
      "2. 1. critique of the Indian Supreme Court\n",
      "3. 2. negative feedback on the Indian Supreme Court\n",
      "4. 3. disapproval of the Indian Supreme Court's decisions\n",
      "\n",
      "=== Top Fused Documents ===\n",
      "Retrieved 10 relevant documents\n",
      "\n",
      "=== Final Answer ===\n",
      "The criticism of the Indian Supreme Court can be summarized as follows:\n",
      "\n",
      "1. Lack of judicial restraint: The Supreme Court has been criticized for unjustifiably trying to perform executive or legislative functions, which is deemed unconstitutional. There have been instances where judges have overstepped their limits in the name of judicial activism.\n",
      "\n",
      "2. Need for judicial independence: While the Constitution has provisions to safeguard the independence of the Supreme Court, there have been concerns about the interference of the executive and legislature. The court should be allowed to function without fear or favor, ensuring impartiality in its decisions.\n",
      "\n",
      "3. Rejection of reforms: The Supreme Court's decision to declare the National Judicial Appointments Commission Act and the 99th Constitutional Amendment as unconstitutional has been met with criticism. The rejection of reforms aimed at changing the system of appointing judges has raised concerns about the judiciary's independence.\n",
      "\n",
      "4. Judicial overreach: The Supreme Court has been cautioned against encroaching on the domains of the executive and legislature. There have been instances where the court has been accused of overreach and adventurism, which can undermine the separation of powers.\n",
      "\n",
      "5. Slow pace of justice: The trial system in India, both on the civil and criminal sides, has been criticized for being inefficient and broken. Delays in delivering justice have been highlighted as a major issue that needs to be addressed.\n",
      "\n",
      "Overall, the criticism of the Indian Supreme Court revolves around issues of judicial restraint, independence, rejection of reforms, judicial overreach, and the slow pace of justice delivery. These criticisms point towards the need for reforms and improvements in the functioning of the judiciary to ensure effective and impartial justice delivery.\n"
     ]
    }
   ],
   "source": [
    "rag_fusion = RAGFusion(index_polity)\n",
    "\n",
    "# Get user query\n",
    "user_query = input(\"Enter your query: \")\n",
    "\n",
    "# Execute fusion pipeline\n",
    "results = rag_fusion.execute_fusion(user_query)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n=== Query Variations ===\")\n",
    "for i, query in enumerate(results['query_variations'], 1):\n",
    "    print(f\"{i}. {query}\")\n",
    "\n",
    "print(\"\\n=== Top Fused Documents ===\")\n",
    "print(f\"Retrieved {len(results['fused_documents'])} relevant documents\")\n",
    "\n",
    "print(\"\\n=== Final Answer ===\")\n",
    "print(results['final_answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8738da5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
